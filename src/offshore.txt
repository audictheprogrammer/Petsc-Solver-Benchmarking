FICHIER: offshore.mtx
#####################################
############# MUMPS LU ##############
#####################################
%%MatrixMarket matrix coordinate real symmetric
M: 259789, N: 259789, nnz: 2251231
Reading matrix completes.
KSP Object: 1 MPI process
  type: gmres
    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
    happy breakdown tolerance 1e-30
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 1 MPI process
  type: lu
    out-of-place factorization
    tolerance for zero pivot 2.22045e-14
    matrix ordering: external
    factor fill ratio given 0., needed 0.
      Factored matrix follows:
        Mat Object: 1 MPI process
          type: mumps
          rows=259789, cols=259789
          package used to perform factorization: mumps
          total: nonzeros=159719571, allocated nonzeros=159719571
            MUMPS run parameters:
              Use -ksp_view ::ascii_info_detail to display information for all processes
              RINFOG(1) (global estimated flops for the elimination after analysis): 1.56511e+11
              RINFOG(2) (global estimated flops for the assembly after factorization): 2.73921e+08
              RINFOG(3) (global estimated flops for the elimination after factorization): 1.56511e+11
              (RINFOG(12) RINFOG(13))*2^INFOG(34) (determinant): (0.,0.)*(2^0)
              INFOG(3) (estimated real workspace for factors on all processors after analysis): 159719571
              INFOG(4) (estimated integer workspace for factors on all processors after analysis): 2784436
              INFOG(5) (estimated maximum front size in the complete tree): 3373
              INFOG(6) (number of nodes in the complete tree): 10721
              INFOG(7) (ordering option effectively used after analysis): 5
              INFOG(8) (structural symmetry in percent of the permuted matrix after analysis): 100
              INFOG(9) (total real/complex workspace to store the matrix factors after factorization): 159719571
              INFOG(10) (total integer space store the matrix factors after factorization): 2784436
              INFOG(11) (order of largest frontal matrix after factorization): 3373
              INFOG(12) (number of off-diagonal pivots): 0
              INFOG(13) (number of delayed pivots after factorization): 0
              INFOG(14) (number of memory compress after factorization): 0
              INFOG(15) (number of steps of iterative refinement after solution): 0
              INFOG(16) (estimated size (in MB) of all MUMPS internal data for factorization after analysis: value on the most memory consuming processor): 1698
              INFOG(17) (estimated size of all MUMPS internal data for factorization after analysis: sum over all processors): 1698
              INFOG(18) (size of all MUMPS internal data allocated during factorization: value on the most memory consuming processor): 1698
              INFOG(19) (size of all MUMPS internal data allocated during factorization: sum over all processors): 1698
              INFOG(20) (estimated number of entries in the factors): 159719571
              INFOG(21) (size in MB of memory effectively used during factorization - value on the most memory consuming processor): 1427
              INFOG(22) (size in MB of memory effectively used during factorization - sum over all processors): 1427
              INFOG(23) (after analysis: value of ICNTL(6) effectively used): 0
              INFOG(24) (after analysis: value of ICNTL(12) effectively used): 1
              INFOG(25) (after factorization: number of pivots modified by static pivoting): 0
              INFOG(28) (after factorization: number of null pivots encountered): 0
              INFOG(29) (after factorization: effective number of entries in the factors (sum over all processors)): 159719571
              INFOG(30, 31) (after solution: size in Mbytes of memory used during solution phase): 1659, 1659
              INFOG(32) (after analysis: type of analysis done): 1
              INFOG(33) (value used for ICNTL(8)): 7
              INFOG(34) (exponent of the determinant if determinant is requested): 0
              INFOG(35) (after factorization: number of entries taking into account BLR factor compression - sum over all processors): 159719571
              INFOG(36) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - value on the most memory consuming processor): 0
              INFOG(37) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - sum over all processors): 0
              INFOG(38) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - value on the most memory consuming processor): 0
              INFOG(39) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - sum over all processors): 0
  linear system matrix = precond matrix:
  Mat Object: 1 MPI process
    type: seqaij
    rows=259789, cols=259789
    total: nonzeros=4242673, allocated nonzeros=4242673
    total number of mallocs used during MatSetValues calls=0
      not using I-node routines
Norm of error 8.85802e-11, Iterations 1
Summary of Memory Usage in PETSc
Maximum (over computational time) process memory:        total 1.5936e+09 max 1.5936e+09 min 1.5936e+09
Current process memory:                                  total 9.5932e+07 max 9.5932e+07 min 9.5932e+07
****************************************************************************************************************************************************************
***                                WIDEN YOUR WINDOW TO 160 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document                                 ***
****************************************************************************************************************************************************************

------------------------------------------------------------------ PETSc Performance Summary: ------------------------------------------------------------------

./ex72 on a arch-opt named DESKTOP-ME2409Q with 1 processor, by xu Wed Mar 13 20:18:21 2024
Using Petsc Release Version 3.20.3, unknown 

                         Max       Max/Min     Avg       Total
Time (sec):           9.641e+01     1.000   9.641e+01
Objects:              0.000e+00     0.000   0.000e+00
Flops:                9.329e+08     1.000   9.329e+08  9.329e+08
Flops/sec:            9.676e+06     1.000   9.676e+06  9.676e+06
MPI Msg Count:        0.000e+00     0.000   0.000e+00  0.000e+00
MPI Msg Len (bytes):  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 9.6414e+01 100.0%  9.3289e+08 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult                2 1.0 3.3906e-02 1.0 1.65e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0   485
MatSolve               2 1.0 5.8645e-01 1.0 6.38e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1 68  0  0  0   1 68  0  0  0  1088
MatLUFactorSym         1 1.0 3.6072e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  4  0  0  0  0   4  0  0  0  0     0
MatLUFactorNum         1 1.0 6.6221e+01 1.0 2.74e+08 1.0 0.0e+00 0.0e+00 0.0e+00 69 29  0  0  0  69 29  0  0  0     4
MatAssemblyBegin       1 1.0 7.0000e-07 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         1 1.0 1.0559e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView                2 1.0 5.1127e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot                1 1.0 5.5790e-04 1.0 5.20e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   931
VecNorm                3 1.0 1.0661e-03 1.0 1.56e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1462
VecScale               2 1.0 6.9440e-04 1.0 5.20e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   748
VecCopy                3 1.0 6.0275e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 2 1.0 9.1152e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                2 1.0 2.2608e-02 1.0 1.04e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0    46
VecMAXPY               2 1.0 7.0290e-04 1.0 1.04e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1478
VecNormalize           2 1.0 1.6654e-03 1.0 1.56e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   936
KSPSetUp               1 1.0 5.7841e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 7.5956e-01 1.0 6.50e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1 70  0  0  0   1 70  0  0  0   855
KSPGMRESOrthog         1 1.0 1.3003e-03 1.0 1.04e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   799
PCSetUp                1 1.0 6.9832e+01 1.0 2.74e+08 1.0 0.0e+00 0.0e+00 0.0e+00 72 29  0  0  0  72 29  0  0  0     4
PCApply                2 1.0 5.8655e-01 1.0 6.38e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1 68  0  0  0   1 68  0  0  0  1087
------------------------------------------------------------------------------------------------------------------------

Object Type          Creations   Destructions. Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3
              Vector     8              8
       Krylov Solver     1              1
      Preconditioner     1              1
              Viewer     2              2
    Distributed Mesh     1              1
   Star Forest Graph     2              2
     Discrete System     1              1
           Weak Form     1              1
========================================================================================================================
Average time to get PetscTime(): 3e-08
#PETSc Option Table entries:
-aij_only # (source: command line)
-fin /mnt/c/Users/xu/petsc/share/petsc/datafiles/matrices/MYMAT/offshore.mtx # (source: command line)
-fout petscmat.aij # (source: command line)
-ksp_view # (source: command line)
-log_view # (source: command line)
-memory_view # (source: command line)
-pc_factor_mat_solver_type mumps # (source: command line)
-pc_type lu # (source: command line)
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-mumps --download-metis --download-parmetis --download-ptscotch --download-superlu --download-strumpack --download-bison --download-scalapack --download-suitesparse --download-cmake --with-debugging=0 PETSC_ARCH=arch-opt
-----------------------------------------
Libraries compiled on 2024-03-11 18:26:13 on DESKTOP-ME2409Q 
Machine characteristics: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
Using PETSc directory: /mnt/c/Users/xu/petsc
Using PETSc arch: arch-opt
-----------------------------------------

Using C compiler: mpicc  -fPIC -Wall -Wwrite-strings -Wno-unknown-pragmas -Wno-lto-type-mismatch -Wno-stringop-overflow -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/mnt/c/Users/xu/petsc/include -I/mnt/c/Users/xu/petsc/arch-opt/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -lpetsc -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -L/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/11 -L/usr/lib/gcc/x86_64-linux-gnu/11 -lspqr -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -ldmumps -lmumps_common -lpord -lpthread -lstrumpack -lscalapack -lsuperlu -llapack -lblas -lptesmumps -lptscotchparmetisv3 -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lparmetis -lmetis -lm -lX11 -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lopen-rte -lopen-pal -lhwloc -levent_core -levent_pthreads -lgfortran -lm -lz -lgfortran -lm -lgfortran -lgcc_s -lquadmath -lstdc++ -lrt -lquadmath
-----------------------------------------



#####################################
############# MUMPS Cho #############
#####################################
%%MatrixMarket matrix coordinate real symmetric
M: 259789, N: 259789, nnz: 2251231
Reading matrix completes.
KSP Object: 1 MPI process
  type: gmres
    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
    happy breakdown tolerance 1e-30
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 1 MPI process
  type: cholesky
    out-of-place factorization
    tolerance for zero pivot 2.22045e-14
    matrix ordering: external
    factor fill ratio given 0., needed 0.
      Factored matrix follows:
        Mat Object: 1 MPI process
          type: mumps
          rows=259789, cols=259789
          package used to perform factorization: mumps
          total: nonzeros=80896444, allocated nonzeros=80896444
            MUMPS run parameters:
              Use -ksp_view ::ascii_info_detail to display information for all processes
              RINFOG(1) (global estimated flops for the elimination after analysis): 8.0213e+10
              RINFOG(2) (global estimated flops for the assembly after factorization): 1.31042e+08
              RINFOG(3) (global estimated flops for the elimination after factorization): 8.0213e+10
              (RINFOG(12) RINFOG(13))*2^INFOG(34) (determinant): (0.,0.)*(2^0)
              INFOG(3) (estimated real workspace for factors on all processors after analysis): 85230301
              INFOG(4) (estimated integer workspace for factors on all processors after analysis): 2736048
              INFOG(5) (estimated maximum front size in the complete tree): 3373
              INFOG(6) (number of nodes in the complete tree): 10280
              INFOG(7) (ordering option effectively used after analysis): 5
              INFOG(8) (structural symmetry in percent of the permuted matrix after analysis): 100
              INFOG(9) (total real/complex workspace to store the matrix factors after factorization): 85230301
              INFOG(10) (total integer space store the matrix factors after factorization): 2736048
              INFOG(11) (order of largest frontal matrix after factorization): 3373
              INFOG(12) (number of off-diagonal pivots): 0
              INFOG(13) (number of delayed pivots after factorization): 0
              INFOG(14) (number of memory compress after factorization): 0
              INFOG(15) (number of steps of iterative refinement after solution): 0
              INFOG(16) (estimated size (in MB) of all MUMPS internal data for factorization after analysis: value on the most memory consuming processor): 948
              INFOG(17) (estimated size of all MUMPS internal data for factorization after analysis: sum over all processors): 948
              INFOG(18) (size of all MUMPS internal data allocated during factorization: value on the most memory consuming processor): 948
              INFOG(19) (size of all MUMPS internal data allocated during factorization: sum over all processors): 948
              INFOG(20) (estimated number of entries in the factors): 80896444
              INFOG(21) (size in MB of memory effectively used during factorization - value on the most memory consuming processor): 798
              INFOG(22) (size in MB of memory effectively used during factorization - sum over all processors): 798
              INFOG(23) (after analysis: value of ICNTL(6) effectively used): 0
              INFOG(24) (after analysis: value of ICNTL(12) effectively used): 1
              INFOG(25) (after factorization: number of pivots modified by static pivoting): 0
              INFOG(28) (after factorization: number of null pivots encountered): 0
              INFOG(29) (after factorization: effective number of entries in the factors (sum over all processors)): 80896444
              INFOG(30, 31) (after solution: size in Mbytes of memory used during solution phase): 931, 931
              INFOG(32) (after analysis: type of analysis done): 1
              INFOG(33) (value used for ICNTL(8)): 7
              INFOG(34) (exponent of the determinant if determinant is requested): 0
              INFOG(35) (after factorization: number of entries taking into account BLR factor compression - sum over all processors): 80896444
              INFOG(36) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - value on the most memory consuming processor): 0
              INFOG(37) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - sum over all processors): 0
              INFOG(38) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - value on the most memory consuming processor): 0
              INFOG(39) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - sum over all processors): 0
  linear system matrix = precond matrix:
  Mat Object: 1 MPI process
    type: seqaij
    rows=259789, cols=259789
    total: nonzeros=4242673, allocated nonzeros=4242673
    total number of mallocs used during MatSetValues calls=0
      not using I-node routines
Norm of error 8.89234e-11, Iterations 1
Summary of Memory Usage in PETSc
Maximum (over computational time) process memory:        total 9.7212e+08 max 9.7212e+08 min 9.7212e+08
Current process memory:                                  total 9.7710e+07 max 9.7710e+07 min 9.7710e+07
****************************************************************************************************************************************************************
***                                WIDEN YOUR WINDOW TO 160 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document                                 ***
****************************************************************************************************************************************************************

------------------------------------------------------------------ PETSc Performance Summary: ------------------------------------------------------------------

./ex72 on a arch-opt named DESKTOP-ME2409Q with 1 processor, by xu Wed Mar 13 20:19:39 2024
Using Petsc Release Version 3.20.3, unknown 

                         Max       Max/Min     Avg       Total
Time (sec):           7.677e+01     1.000   7.677e+01
Objects:              0.000e+00     0.000   0.000e+00
Flops:                4.747e+08     1.000   4.747e+08  4.747e+08
Flops/sec:            6.184e+06     1.000   6.184e+06  6.184e+06
MPI Msg Count:        0.000e+00     0.000   0.000e+00  0.000e+00
MPI Msg Len (bytes):  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 7.6771e+01 100.0%  4.7472e+08 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult                2 1.0 1.5398e-02 1.0 1.65e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  3  0  0  0  1068
MatSolve               2 1.0 4.6008e-01 1.0 3.23e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1 68  0  0  0   1 68  0  0  0   701
MatCholFctrSym         1 1.0 4.2143e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  5  0  0  0  0   5  0  0  0  0     0
MatCholFctrNum         1 1.0 3.9967e+01 1.0 1.31e+08 1.0 0.0e+00 0.0e+00 0.0e+00 52 28  0  0  0  52 28  0  0  0     3
MatAssemblyBegin       1 1.0 4.1000e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         1 1.0 2.8661e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView                2 1.0 5.7273e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot                1 1.0 4.1010e-04 1.0 5.20e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1267
VecNorm                3 1.0 9.8960e-04 1.0 1.56e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1575
VecScale               2 1.0 3.0320e-04 1.0 5.20e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1714
VecCopy                3 1.0 1.3490e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 2 1.0 3.3900e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                2 1.0 9.5640e-04 1.0 1.04e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1087
VecMAXPY               2 1.0 5.5140e-04 1.0 1.04e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1885
VecNormalize           2 1.0 9.5640e-04 1.0 1.56e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1630
KSPSetUp               1 1.0 5.4898e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 5.4876e-01 1.0 3.34e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1 70  0  0  0   1 70  0  0  0   609
KSPGMRESOrthog         1 1.0 7.6150e-04 1.0 1.04e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1365
PCSetUp                1 1.0 4.4200e+01 1.0 1.31e+08 1.0 0.0e+00 0.0e+00 0.0e+00 58 28  0  0  0  58 28  0  0  0     3
PCApply                2 1.0 4.6009e-01 1.0 3.23e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1 68  0  0  0   1 68  0  0  0   701
------------------------------------------------------------------------------------------------------------------------

Object Type          Creations   Destructions. Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3
              Vector     8              8
       Krylov Solver     1              1
      Preconditioner     1              1
              Viewer     2              2
    Distributed Mesh     1              1
   Star Forest Graph     2              2
     Discrete System     1              1
           Weak Form     1              1
========================================================================================================================
Average time to get PetscTime(): 3e-08
#PETSc Option Table entries:
-aij_only # (source: command line)
-fin /mnt/c/Users/xu/petsc/share/petsc/datafiles/matrices/MYMAT/offshore.mtx # (source: command line)
-fout petscmat.aij # (source: command line)
-ksp_view # (source: command line)
-log_view # (source: command line)
-memory_view # (source: command line)
-pc_factor_mat_solver_type mumps # (source: command line)
-pc_type cholesky # (source: command line)
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-mumps --download-metis --download-parmetis --download-ptscotch --download-superlu --download-strumpack --download-bison --download-scalapack --download-suitesparse --download-cmake --with-debugging=0 PETSC_ARCH=arch-opt
-----------------------------------------
Libraries compiled on 2024-03-11 18:26:13 on DESKTOP-ME2409Q 
Machine characteristics: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
Using PETSc directory: /mnt/c/Users/xu/petsc
Using PETSc arch: arch-opt
-----------------------------------------

Using C compiler: mpicc  -fPIC -Wall -Wwrite-strings -Wno-unknown-pragmas -Wno-lto-type-mismatch -Wno-stringop-overflow -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/mnt/c/Users/xu/petsc/include -I/mnt/c/Users/xu/petsc/arch-opt/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -lpetsc -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -L/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/11 -L/usr/lib/gcc/x86_64-linux-gnu/11 -lspqr -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -ldmumps -lmumps_common -lpord -lpthread -lstrumpack -lscalapack -lsuperlu -llapack -lblas -lptesmumps -lptscotchparmetisv3 -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lparmetis -lmetis -lm -lX11 -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lopen-rte -lopen-pal -lhwloc -levent_core -levent_pthreads -lgfortran -lm -lz -lgfortran -lm -lgfortran -lgcc_s -lquadmath -lstdc++ -lrt -lquadmath
-----------------------------------------



#####################################
############# SuperLU LU ############
#####################################
%%MatrixMarket matrix coordinate real symmetric
M: 259789, N: 259789, nnz: 2251231
Reading matrix completes.


#####################################
############ Strumpack LU ###########
#####################################
%%MatrixMarket matrix coordinate real symmetric
M: 259789, N: 259789, nnz: 2251231
Reading matrix completes.
KSP Object: 1 MPI process
  type: gmres
    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
    happy breakdown tolerance 1e-30
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 1 MPI process
  type: lu
    out-of-place factorization
    tolerance for zero pivot 2.22045e-14
    matrix ordering: external
    factor fill ratio given 0., needed 0.
      Factored matrix follows:
        Mat Object: 1 MPI process
          type: strumpack
          rows=259789, cols=259789
          package used to perform factorization: strumpack
          total: nonzeros=0, allocated nonzeros=0
            STRUMPACK sparse solver!
  linear system matrix = precond matrix:
  Mat Object: 1 MPI process
    type: seqaij
    rows=259789, cols=259789
    total: nonzeros=4242673, allocated nonzeros=4242673
    total number of mallocs used during MatSetValues calls=0
      not using I-node routines
Norm of error 8.78257e-11, Iterations 1
Summary of Memory Usage in PETSc
Maximum (over computational time) process memory:        total 1.6563e+09 max 1.6563e+09 min 1.6563e+09
Current process memory:                                  total 1.4819e+09 max 1.4819e+09 min 1.4819e+09
****************************************************************************************************************************************************************
***                                WIDEN YOUR WINDOW TO 160 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document                                 ***
****************************************************************************************************************************************************************

------------------------------------------------------------------ PETSc Performance Summary: ------------------------------------------------------------------

./ex72 on a arch-opt named DESKTOP-ME2409Q with 1 processor, by xu Wed Mar 13 20:26:50 2024
Using Petsc Release Version 3.20.3, unknown 

                         Max       Max/Min     Avg       Total
Time (sec):           1.002e+02     1.000   1.002e+02
Objects:              0.000e+00     0.000   0.000e+00
Flops:                2.113e+07     1.000   2.113e+07  2.113e+07
Flops/sec:            2.108e+05     1.000   2.108e+05  2.108e+05
MPI Msg Count:        0.000e+00     0.000   0.000e+00  0.000e+00
MPI Msg Len (bytes):  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.0023e+02 100.0%  2.1127e+07 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult                2 1.0 2.1714e-02 1.0 1.65e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0 78  0  0  0   0 78  0  0  0   758
MatSolve               2 1.0 4.7004e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactor            1 1.0 7.1585e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 71  0  0  0  0  71  0  0  0  0     0
MatAssemblyBegin       1 1.0 2.0000e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         1 1.0 1.8053e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            1 1.0 6.0000e-07 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView                2 1.0 8.4603e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot                1 1.0 1.7020e-04 1.0 5.20e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  3053
VecNorm                3 1.0 1.1441e-03 1.0 1.56e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1362
VecScale               2 1.0 2.0230e-04 1.0 5.20e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  2568
VecCopy                1 1.0 3.6700e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 2 1.0 6.1250e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                2 1.0 7.4470e-04 1.0 1.04e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  5  0  0  0   0  5  0  0  0  1395
VecMAXPY               2 1.0 3.3680e-04 1.0 1.04e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  5  0  0  0   0  5  0  0  0  3085
VecNormalize           2 1.0 8.4440e-04 1.0 1.56e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  7  0  0  0   0  7  0  0  0  1846
KSPSetUp               1 1.0 2.0363e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 5.9495e-01 1.0 1.19e+07 1.0 0.0e+00 0.0e+00 0.0e+00  1 56  0  0  0   1 56  0  0  0    20
KSPGMRESOrthog         1 1.0 3.5150e-04 1.0 1.04e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  5  0  0  0   0  5  0  0  0  2956
PCSetUp                1 1.0 7.1585e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 71  0  0  0  0  71  0  0  0  0     0
PCApply                2 1.0 4.7004e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Object Type          Creations   Destructions. Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3
              Vector     8              8
       Krylov Solver     1              1
      Preconditioner     1              1
              Viewer     2              2
    Distributed Mesh     1              1
   Star Forest Graph     2              2
     Discrete System     1              1
           Weak Form     1              1
========================================================================================================================
Average time to get PetscTime(): 3e-08
#PETSc Option Table entries:
-aij_only # (source: command line)
-fin /mnt/c/Users/xu/petsc/share/petsc/datafiles/matrices/MYMAT/offshore.mtx # (source: command line)
-fout petscmat.aij # (source: command line)
-ksp_view # (source: command line)
-log_view # (source: command line)
-memory_view # (source: command line)
-pc_factor_mat_solver_type strumpack # (source: command line)
-pc_type lu # (source: command line)
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-mumps --download-metis --download-parmetis --download-ptscotch --download-superlu --download-strumpack --download-bison --download-scalapack --download-suitesparse --download-cmake --with-debugging=0 PETSC_ARCH=arch-opt
-----------------------------------------
Libraries compiled on 2024-03-11 18:26:13 on DESKTOP-ME2409Q 
Machine characteristics: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
Using PETSc directory: /mnt/c/Users/xu/petsc
Using PETSc arch: arch-opt
-----------------------------------------

Using C compiler: mpicc  -fPIC -Wall -Wwrite-strings -Wno-unknown-pragmas -Wno-lto-type-mismatch -Wno-stringop-overflow -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/mnt/c/Users/xu/petsc/include -I/mnt/c/Users/xu/petsc/arch-opt/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -lpetsc -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -L/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/11 -L/usr/lib/gcc/x86_64-linux-gnu/11 -lspqr -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -ldmumps -lmumps_common -lpord -lpthread -lstrumpack -lscalapack -lsuperlu -llapack -lblas -lptesmumps -lptscotchparmetisv3 -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lparmetis -lmetis -lm -lX11 -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lopen-rte -lopen-pal -lhwloc -levent_core -levent_pthreads -lgfortran -lm -lz -lgfortran -lm -lgfortran -lgcc_s -lquadmath -lstdc++ -lrt -lquadmath
-----------------------------------------




#####################################
########### SuiteSparse LU ##########
#####################################
%%MatrixMarket matrix coordinate real symmetric
M: 259789, N: 259789, nnz: 2251231
Reading matrix completes.


#####################################
########### SuiteSparse Cho #########
#####################################
%%MatrixMarket matrix coordinate real symmetric
M: 259789, N: 259789, nnz: 2251231
Reading matrix completes.
KSP Object: 1 MPI process
  type: gmres
    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
    happy breakdown tolerance 1e-30
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 1 MPI process
  type: cholesky
    out-of-place factorization
    tolerance for zero pivot 2.22045e-14
    matrix ordering: external
    factor fill ratio given 0., needed 0.
      Factored matrix follows:
        Mat Object: 1 MPI process
          type: cholmod
          rows=259789, cols=259789
          package used to perform factorization: cholmod
          total: nonzeros=75193703, allocated nonzeros=75193703
            CHOLMOD run parameters:
              Pack factors after symbolic factorization: TRUE
              Common.dbound            0.  (Smallest absolute value of diagonal entries of D)
              Common.grow0             1.2
              Common.grow1             1.2
              Common.grow2             5
              Common.maxrank           8
              Common.supernodal_switch 40.
              Common.supernodal        1
              Common.final_asis        1
              Common.final_super       1
              Common.final_ll          0
              Common.final_pack        1
              Common.final_monotonic   1
              Common.final_resymbol    0
              Common.zrelax            [0.8,0.1,0.05]
              Common.nrelax            [4,16,48]
              Common.prefer_upper      1
              Common.print             3
              Common.postorder         1
              Common.default_nesdis    0 (use NESDIS instead of METIS for nested dissection)
              Common.fl                8.17361e+10 (flop count from most recent analysis)
              Common.lnz               7.51937e+07 (fundamental nz in L)
              Common.anz               2.25123e+06
              Common.modfl             -1. (flop count from most recent update)
              Common.malloc_count      12. (number of live objects)
              Common.memory_usage      8.21293e+08 (peak memory usage in bytes)
              Common.memory_inuse      7.48725e+08 (current memory usage in bytes)
              Common.nrealloc_col      0. (number of column reallocations)
              Common.nrealloc_factor   0. (number of factor reallocations due to column reallocations)
              Common.ndbounds_hit      0. (number of times diagonal was modified by dbound)
              Common.rowfacfl          0. (number of flops in last call to cholmod_rowfac)
              Common.aatfl             0. (number of flops to compute A(:,f)*A(:,f)')
  linear system matrix = precond matrix:
  Mat Object: 1 MPI process
    type: seqaij
    rows=259789, cols=259789
    total: nonzeros=4242673, allocated nonzeros=4242673
    total number of mallocs used during MatSetValues calls=0
      not using I-node routines
Norm of error 8.80151e-11, Iterations 1
Summary of Memory Usage in PETSc
Maximum (over computational time) process memory:        total 9.6067e+08 max 9.6067e+08 min 9.6067e+08
Current process memory:                                  total 1.5235e+08 max 1.5235e+08 min 1.5235e+08
****************************************************************************************************************************************************************
***                                WIDEN YOUR WINDOW TO 160 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document                                 ***
****************************************************************************************************************************************************************

------------------------------------------------------------------ PETSc Performance Summary: ------------------------------------------------------------------

./ex72 on a arch-opt named DESKTOP-ME2409Q with 1 processor, by xu Wed Mar 13 20:30:47 2024
Using Petsc Release Version 3.20.3, unknown 

                         Max       Max/Min     Avg       Total
Time (sec):           6.170e+01     1.000   6.170e+01
Objects:              0.000e+00     0.000   0.000e+00
Flops:                8.236e+10     1.000   8.236e+10  8.236e+10
Flops/sec:            1.335e+09     1.000   1.335e+09  1.335e+09
MPI Msg Count:        0.000e+00     0.000   0.000e+00  0.000e+00
MPI Msg Len (bytes):  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 6.1698e+01 100.0%  8.2359e+10 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult                2 1.0 1.2936e-02 1.0 1.65e+07 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1272
MatSolve               2 1.0 3.8743e-01 1.0 6.02e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0  1553
MatCholFctrSym         1 1.0 3.7352e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  6  0  0  0  0   6  0  0  0  0     0
MatCholFctrNum         1 1.0 3.4206e+01 1.0 8.17e+10 1.0 0.0e+00 0.0e+00 0.0e+00 55 99  0  0  0  55 99  0  0  0  2390
MatAssemblyBegin       1 1.0 1.4000e-06 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         1 1.0 9.0695e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView                2 1.0 4.8051e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot                1 1.0 2.0070e-04 1.0 5.20e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2589
VecNorm                3 1.0 9.5690e-04 1.0 1.56e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1629
VecScale               2 1.0 2.1510e-04 1.0 5.20e+05 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2416
VecCopy                1 1.0 6.2750e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 2 1.0 3.4070e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                2 1.0 1.3152e-03 1.0 1.04e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   790
VecMAXPY               2 1.0 4.3780e-04 1.0 1.04e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2374
VecNormalize           2 1.0 8.5440e-04 1.0 1.56e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1824
KSPSetUp               1 1.0 5.8355e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 4.6596e-01 1.0 6.13e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0  1316
KSPGMRESOrthog         1 1.0 4.3840e-04 1.0 1.04e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2370
PCSetUp                1 1.0 3.7952e+01 1.0 8.17e+10 1.0 0.0e+00 0.0e+00 0.0e+00 62 99  0  0  0  62 99  0  0  0  2154
PCApply                2 1.0 3.8744e-01 1.0 6.02e+08 1.0 0.0e+00 0.0e+00 0.0e+00  1  1  0  0  0   1  1  0  0  0  1553
------------------------------------------------------------------------------------------------------------------------

Object Type          Creations   Destructions. Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     3              3
              Vector     8              8
       Krylov Solver     1              1
      Preconditioner     1              1
              Viewer     2              2
    Distributed Mesh     1              1
   Star Forest Graph     2              2
     Discrete System     1              1
           Weak Form     1              1
========================================================================================================================
Average time to get PetscTime(): 3e-08
#PETSc Option Table entries:
-aij_only # (source: command line)
-fin /mnt/c/Users/xu/petsc/share/petsc/datafiles/matrices/MYMAT/offshore.mtx # (source: command line)
-fout petscmat.aij # (source: command line)
-ksp_view # (source: command line)
-log_view # (source: command line)
-memory_view # (source: command line)
-pc_factor_mat_solver_type cholmod # (source: command line)
-pc_type cholesky # (source: command line)
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-mumps --download-metis --download-parmetis --download-ptscotch --download-superlu --download-strumpack --download-bison --download-scalapack --download-suitesparse --download-cmake --with-debugging=0 PETSC_ARCH=arch-opt
-----------------------------------------
Libraries compiled on 2024-03-11 18:26:13 on DESKTOP-ME2409Q 
Machine characteristics: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
Using PETSc directory: /mnt/c/Users/xu/petsc
Using PETSc arch: arch-opt
-----------------------------------------

Using C compiler: mpicc  -fPIC -Wall -Wwrite-strings -Wno-unknown-pragmas -Wno-lto-type-mismatch -Wno-stringop-overflow -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/mnt/c/Users/xu/petsc/include -I/mnt/c/Users/xu/petsc/arch-opt/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -lpetsc -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -L/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/11 -L/usr/lib/gcc/x86_64-linux-gnu/11 -lspqr -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -ldmumps -lmumps_common -lpord -lpthread -lstrumpack -lscalapack -lsuperlu -llapack -lblas -lptesmumps -lptscotchparmetisv3 -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lparmetis -lmetis -lm -lX11 -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lopen-rte -lopen-pal -lhwloc -levent_core -levent_pthreads -lgfortran -lm -lz -lgfortran -lm -lgfortran -lgcc_s -lquadmath -lstdc++ -lrt -lquadmath
-----------------------------------------



#####################################
########### SuiteSparse QR ##########
#####################################
%%MatrixMarket matrix coordinate real symmetric
M: 259789, N: 259789, nnz: 2251231
Reading matrix completes.
CHOLMOD error: out of memory. file: ../Core/cholmod_memory.c line: 146
CHOLMOD error: out of memory. file: ../Core/cholmod_memory.c line: 146


#####################################
############## Petsc LU #############
#####################################
%%MatrixMarket matrix coordinate real symmetric
M: 259789, N: 259789, nnz: 2251231
Reading matrix completes.


######################################
############## Petsc Cho #############
######################################
%%MatrixMarket matrix coordinate real symmetric
M: 259789, N: 259789, nnz: 2251231
Reading matrix completes.
