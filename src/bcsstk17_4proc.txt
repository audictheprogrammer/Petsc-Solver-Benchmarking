FICHIER: bcsstk17.mtx
#####################################
############# MUMPS LU ##############
#####################################
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
Reading matrix completes.
KSP Object: 4 MPI processes
  type: gmres
    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
    happy breakdown tolerance 1e-30
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 4 MPI processes
  type: lu
    out-of-place factorization
    tolerance for zero pivot 2.22045e-14
    matrix ordering: external
    factor fill ratio given 0., needed 0.
      Factored matrix follows:
        Mat Object: 4 MPI processes
          type: mumps
          rows=10974, cols=10974
          package used to perform factorization: mumps
          total: nonzeros=2450620, allocated nonzeros=2450620
            MUMPS run parameters:
              Use -ksp_view ::ascii_info_detail to display information for all processes
              RINFOG(1) (global estimated flops for the elimination after analysis): 3.9147e+08
              RINFOG(2) (global estimated flops for the assembly after factorization): 2.87832e+06
              RINFOG(3) (global estimated flops for the elimination after factorization): 3.9147e+08
              (RINFOG(12) RINFOG(13))*2^INFOG(34) (determinant): (0.,0.)*(2^0)
              INFOG(3) (estimated real workspace for factors on all processors after analysis): 2450620
              INFOG(4) (estimated integer workspace for factors on all processors after analysis): 101182
              INFOG(5) (estimated maximum front size in the complete tree): 477
              INFOG(6) (number of nodes in the complete tree): 950
              INFOG(7) (ordering option effectively used after analysis): 5
              INFOG(8) (structural symmetry in percent of the permuted matrix after analysis): -1
              INFOG(9) (total real/complex workspace to store the matrix factors after factorization): 2450620
              INFOG(10) (total integer space store the matrix factors after factorization): 101182
              INFOG(11) (order of largest frontal matrix after factorization): 477
              INFOG(12) (number of off-diagonal pivots): 0
              INFOG(13) (number of delayed pivots after factorization): 0
              INFOG(14) (number of memory compress after factorization): 0
              INFOG(15) (number of steps of iterative refinement after solution): 0
              INFOG(16) (estimated size (in MB) of all MUMPS internal data for factorization after analysis: value on the most memory consuming processor): 32
              INFOG(17) (estimated size of all MUMPS internal data for factorization after analysis: sum over all processors): 120
              INFOG(18) (size of all MUMPS internal data allocated during factorization: value on the most memory consuming processor): 32
              INFOG(19) (size of all MUMPS internal data allocated during factorization: sum over all processors): 120
              INFOG(20) (estimated number of entries in the factors): 2450620
              INFOG(21) (size in MB of memory effectively used during factorization - value on the most memory consuming processor): 30
              INFOG(22) (size in MB of memory effectively used during factorization - sum over all processors): 114
              INFOG(23) (after analysis: value of ICNTL(6) effectively used): 0
              INFOG(24) (after analysis: value of ICNTL(12) effectively used): 1
              INFOG(25) (after factorization: number of pivots modified by static pivoting): 0
              INFOG(28) (after factorization: number of null pivots encountered): 0
              INFOG(29) (after factorization: effective number of entries in the factors (sum over all processors)): 2450620
              INFOG(30, 31) (after solution: size in Mbytes of memory used during solution phase): 17, 63
              INFOG(32) (after analysis: type of analysis done): 1
              INFOG(33) (value used for ICNTL(8)): 7
              INFOG(34) (exponent of the determinant if determinant is requested): 0
              INFOG(35) (after factorization: number of entries taking into account BLR factor compression - sum over all processors): 2450620
              INFOG(36) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - value on the most memory consuming processor): 0
              INFOG(37) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - sum over all processors): 0
              INFOG(38) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - value on the most memory consuming processor): 0
              INFOG(39) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - sum over all processors): 0
  linear system matrix = precond matrix:
  Mat Object: 4 MPI processes
    type: mpiaij
    rows=10974, cols=10974
    total: nonzeros=219812, allocated nonzeros=219812
    total number of mallocs used during MatSetValues calls=0
      not using I-node (on process 0) routines
Norm of error 1.26311e-12, Iterations 1
Norm of error 1.26311e-12, Iterations 1
Norm of error 1.26311e-12, Iterations 1
Norm of error 1.26311e-12, Iterations 1
Summary of Memory Usage in PETSc
Maximum (over computational time) process memory:        total 2.1179e+08 max 5.6259e+07 min 5.0041e+07
Current process memory:                                  total 1.7745e+08 max 4.9254e+07 min 4.0194e+07
****************************************************************************************************************************************************************
***                                WIDEN YOUR WINDOW TO 160 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document                                 ***
****************************************************************************************************************************************************************

------------------------------------------------------------------ PETSc Performance Summary: ------------------------------------------------------------------

./ex23 on a arch-opt named DESKTOP-ME2409Q with 4 processors, by xu Sun Apr 21 22:14:49 2024
Using Petsc Release Version 3.20.5, unknown 

                         Max       Max/Min     Avg       Total
Time (sec):           2.833e+00     1.000   2.833e+00
Objects:              0.000e+00     0.000   0.000e+00
Flops:                4.345e+06     1.573   3.423e+06  1.369e+07
Flops/sec:            1.534e+06     1.573   1.208e+06  4.833e+06
MPI Msg Count:        2.250e+01     1.250   2.025e+01  8.100e+01
MPI Msg Len (bytes):  9.645e+04     1.510   3.691e+03  2.990e+05
MPI Reductions:       4.600e+01     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.8331e+00 100.0%  1.3692e+07 100.0%  8.100e+01 100.0%  3.691e+03      100.0%  2.800e+01  60.9%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          4 1.0 6.7444e-03 67.6 0.00e+00 0.0 1.2e+01 4.0e+00 4.0e+00  0  0 15  0  9   0  0 15  0 14     0
BuildTwoSidedF         2 1.0 6.7108e-03 164.1 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  4   0  0  0  0  7     0
MatMult                2 1.0 1.6832e-02 35.0 2.53e+05 1.4 6.0e+00 2.1e+03 0.0e+00  0  6  7  4  0   0  6  7  4  0    51
MatSolve               2 1.0 7.6572e-03 1.0 3.22e+06 1.8 6.9e+01 4.1e+03 5.0e+00  0 71 85 95 11   0 71 85 95 18  1274
MatLUFactorSym         1 1.0 7.7358e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  3  0  0  0  0   3  0  0  0  0     0
MatLUFactorNum         1 1.0 3.9954e-02 1.0 8.58e+05 1.4 0.0e+00 0.0e+00 0.0e+00  1 21  0  0  0   1 21  0  0  0    72
MatAssemblyBegin       1 1.0 3.5693e-03 85.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  4     0
MatAssemblyEnd         1 1.0 7.1737e-03 1.0 0.00e+00 0.0 6.0e+00 5.3e+02 7.0e+00  0  0  7  1 15   0  0  7  1 25     0
MatView                2 1.0 3.9788e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  4     0
VecMDot                1 1.0 1.9000e-05 2.8 5.49e+03 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  4  1155
VecNorm                3 1.0 1.5400e-04 5.0 1.65e+04 1.0 0.0e+00 0.0e+00 3.0e+00  0  0  0  0  7   0  0  0  0 11   427
VecScale               2 1.0 5.2000e-06 1.3 5.49e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4221
VecCopy                1 1.0 1.3300e-05 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 2 1.0 1.0200e-05 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                2 1.0 1.5700e-05 1.4 1.10e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2796
VecMAXPY               2 1.0 5.7000e-06 1.6 1.10e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  7701
VecScatterBegin        4 1.0 1.2360e-04 1.8 0.00e+00 0.0 2.4e+01 7.0e+03 0.0e+00  0  0 30 56  0   0  0 30 56  0     0
VecScatterEnd          4 1.0 1.6712e-02 51.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize           2 1.0 3.5100e-05 1.4 1.65e+04 1.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  4   0  0  0  0  7  1876
SFSetGraph             2 1.0 1.7790e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                2 1.0 1.8500e-04 1.2 0.00e+00 0.0 2.4e+01 1.8e+03 2.0e+00  0  0 30 14  4   0  0 30 14  7     0
SFPack                 4 1.0 3.4300e-05 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack               4 1.0 1.5400e-05 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               1 1.0 1.2310e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 8.6025e-03 1.0 3.37e+06 1.7 7.2e+01 4.0e+03 8.0e+00  0 76 89 97 17   0 76 89 97 29  1202
KSPGMRESOrthog         1 1.0 2.9000e-05 1.9 1.10e+04 1.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  4  1514
PCSetUp                1 1.0 1.1741e-01 1.0 8.58e+05 1.4 0.0e+00 0.0e+00 2.0e+00  4 21  0  0  4   4 21  0  0  7    25
PCApply                2 1.0 7.6625e-03 1.0 3.22e+06 1.8 6.9e+01 4.1e+03 5.0e+00  0 71 85 95 11   0 71 85 95 18  1274
------------------------------------------------------------------------------------------------------------------------

Object Type          Creations   Destructions. Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     5              5
              Vector    12             12
           Index Set     4              4
   Star Forest Graph     4              4
       Krylov Solver     1              1
      Preconditioner     1              1
              Viewer     1              1
    Distributed Mesh     1              1
     Discrete System     1              1
           Weak Form     1              1
========================================================================================================================
Average time to get PetscTime(): 3e-08
Average time for MPI_Barrier(): 2.91e-05
Average time for zero size MPI_Send(): 3.275e-06
#PETSc Option Table entries:
-aij_only # (source: command line)
-fin /mnt/c/Users/xu/petsc/share/petsc/datafiles/matrices/MYMAT/bcsstk17.mtx # (source: command line)
-ksp_view # (source: command line)
-log_view # (source: command line)
-memory_view # (source: command line)
-pc_factor_mat_solver_type mumps # (source: command line)
-pc_type lu # (source: command line)
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-mumps --download-metis --download-parmetis --download-ptscotch --download-superlu --download-superlu_dist --download-strumpack --download-bison --download-scalapack --download-suitesparse --download-cmake --download-fblaslapack --with-debugging=0 PETSC_ARCH=arch-opt
-----------------------------------------
Libraries compiled on 2024-04-15 09:54:45 on DESKTOP-ME2409Q 
Machine characteristics: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
Using PETSc directory: /mnt/c/Users/xu/petsc
Using PETSc arch: arch-opt
-----------------------------------------

Using C compiler: mpicc  -fPIC -Wall -Wwrite-strings -Wno-unknown-pragmas -Wno-lto-type-mismatch -Wno-stringop-overflow -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/mnt/c/Users/xu/petsc/include -I/mnt/c/Users/xu/petsc/arch-opt/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -lpetsc -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -L/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/11 -L/usr/lib/gcc/x86_64-linux-gnu/11 -lspqr -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -ldmumps -lmumps_common -lpord -lpthread -lstrumpack -lscalapack -lsuperlu -lsuperlu_dist -lflapack -lfblas -lptesmumps -lptscotchparmetisv3 -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lparmetis -lmetis -lm -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lopen-rte -lopen-pal -lhwloc -levent_core -levent_pthreads -lgfortran -lm -lz -lgfortran -lm -lgfortran -lgcc_s -lquadmath -lstdc++ -lrt -lquadmath
-----------------------------------------



#####################################
############# MUMPS Cho #############
#####################################
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
Reading matrix completes.
KSP Object: 4 MPI processes
  type: gmres
    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
    happy breakdown tolerance 1e-30
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 4 MPI processes
  type: cholesky
    out-of-place factorization
    tolerance for zero pivot 2.22045e-14
    matrix ordering: external
    factor fill ratio given 0., needed 0.
      Factored matrix follows:
        Mat Object: 4 MPI processes
          type: mumps
          rows=10974, cols=10974
          package used to perform factorization: mumps
          total: nonzeros=10974, allocated nonzeros=10974
            MUMPS run parameters:
              Use -ksp_view ::ascii_info_detail to display information for all processes
              RINFOG(1) (global estimated flops for the elimination after analysis): 0.
              RINFOG(2) (global estimated flops for the assembly after factorization): 0.
              RINFOG(3) (global estimated flops for the elimination after factorization): 0.
              (RINFOG(12) RINFOG(13))*2^INFOG(34) (determinant): (0.,0.)*(2^0)
              INFOG(3) (estimated real workspace for factors on all processors after analysis): 10974
              INFOG(4) (estimated integer workspace for factors on all processors after analysis): 241428
              INFOG(5) (estimated maximum front size in the complete tree): 1
              INFOG(6) (number of nodes in the complete tree): 10974
              INFOG(7) (ordering option effectively used after analysis): 5
              INFOG(8) (structural symmetry in percent of the permuted matrix after analysis): -1
              INFOG(9) (total real/complex workspace to store the matrix factors after factorization): 10974
              INFOG(10) (total integer space store the matrix factors after factorization): 241428
              INFOG(11) (order of largest frontal matrix after factorization): 1
              INFOG(12) (number of off-diagonal pivots): 0
              INFOG(13) (number of delayed pivots after factorization): 0
              INFOG(14) (number of memory compress after factorization): 0
              INFOG(15) (number of steps of iterative refinement after solution): 0
              INFOG(16) (estimated size (in MB) of all MUMPS internal data for factorization after analysis: value on the most memory consuming processor): 3
              INFOG(17) (estimated size of all MUMPS internal data for factorization after analysis: sum over all processors): 9
              INFOG(18) (size of all MUMPS internal data allocated during factorization: value on the most memory consuming processor): 3
              INFOG(19) (size of all MUMPS internal data allocated during factorization: sum over all processors): 9
              INFOG(20) (estimated number of entries in the factors): 10974
              INFOG(21) (size in MB of memory effectively used during factorization - value on the most memory consuming processor): 3
              INFOG(22) (size in MB of memory effectively used during factorization - sum over all processors): 9
              INFOG(23) (after analysis: value of ICNTL(6) effectively used): 0
              INFOG(24) (after analysis: value of ICNTL(12) effectively used): 1
              INFOG(25) (after factorization: number of pivots modified by static pivoting): 0
              INFOG(28) (after factorization: number of null pivots encountered): 0
              INFOG(29) (after factorization: effective number of entries in the factors (sum over all processors)): 10974
              INFOG(30, 31) (after solution: size in Mbytes of memory used during solution phase): 11, 38
              INFOG(32) (after analysis: type of analysis done): 1
              INFOG(33) (value used for ICNTL(8)): 7
              INFOG(34) (exponent of the determinant if determinant is requested): 0
              INFOG(35) (after factorization: number of entries taking into account BLR factor compression - sum over all processors): 10974
              INFOG(36) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - value on the most memory consuming processor): 0
              INFOG(37) (after analysis: estimated size of all MUMPS internal data for running BLR in-core - sum over all processors): 0
              INFOG(38) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - value on the most memory consuming processor): 0
              INFOG(39) (after analysis: estimated size of all MUMPS internal data for running BLR out-of-core - sum over all processors): 0
  linear system matrix = precond matrix:
  Mat Object: 4 MPI processes
    type: mpiaij
    rows=10974, cols=10974
    total: nonzeros=219812, allocated nonzeros=219812
    total number of mallocs used during MatSetValues calls=0
      not using I-node (on process 0) routines
Norm of error 0.00141351, Iterations 27
Norm of error 0.00141351, Iterations 27
Norm of error 0.00141351, Iterations 27
Norm of error 0.00141351, Iterations 27
Summary of Memory Usage in PETSc
Maximum (over computational time) process memory:        total 1.6388e+08 max 4.3119e+07 min 3.9494e+07
Current process memory:                                  total 1.6295e+08 max 4.3119e+07 min 3.9113e+07
****************************************************************************************************************************************************************
***                                WIDEN YOUR WINDOW TO 160 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document                                 ***
****************************************************************************************************************************************************************

------------------------------------------------------------------ PETSc Performance Summary: ------------------------------------------------------------------

./ex23 on a arch-opt named DESKTOP-ME2409Q with 4 processors, by xu Sun Apr 21 22:14:52 2024
Using Petsc Release Version 3.20.5, unknown 

                         Max       Max/Min     Avg       Total
Time (sec):           2.965e+00     1.000   2.964e+00
Objects:              0.000e+00     0.000   0.000e+00
Flops:                8.092e+06     1.151   7.659e+06  3.063e+07
Flops/sec:            2.730e+06     1.151   2.583e+06  1.033e+07
MPI Msg Count:        6.750e+01     2.143   4.800e+01  1.920e+02
MPI Msg Len (bytes):  1.019e+06     2.891   1.116e+04  2.143e+06
MPI Reductions:       9.800e+01     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.9645e+00 100.0%  3.0634e+07 100.0%  1.920e+02 100.0%  1.116e+04      100.0%  8.000e+01  81.6%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          4 1.0 8.7499e-03 11.6 0.00e+00 0.0 6.0e+00 4.0e+00 4.0e+00  0  0  3  0  4   0  0  3  0  5     0
BuildTwoSidedF         2 1.0 8.7286e-03 12.5 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  2   0  0  0  0  2     0
MatMult               28 1.0 9.6905e-03 1.3 3.55e+06 1.4 8.4e+01 2.1e+03 0.0e+00  0 39 44  8  0   0 39 44  8  0  1239
MatSolve              28 1.0 3.4130e-01 1.0 4.61e+05 0.0 1.0e+02 1.9e+04 5.0e+00 11  2 53 92  5  11  2 53 92  6     1
MatCholFctrSym         1 1.0 1.6992e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatCholFctrNum         1 1.0 1.5072e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatAssemblyBegin       1 1.0 7.0318e-03 138.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  1   0  0  0  0  1     0
MatAssemblyEnd         1 1.0 6.6322e-03 1.0 0.00e+00 0.0 6.0e+00 5.3e+02 7.0e+00  0  0  3  0  7   0  0  3  0  9     0
MatView                2 1.0 3.3469e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  1   0  0  0  0  1     0
VecMDot               27 1.0 3.5287e-03 1.1 2.07e+06 1.0 0.0e+00 0.0e+00 2.7e+01  0 27  0  0 28   0 27  0  0 34  2351
VecNorm               29 1.0 1.1166e-03 1.7 1.59e+05 1.0 0.0e+00 0.0e+00 2.9e+01  0  2  0  0 30   0  2  0  0 36   570
VecScale              28 1.0 6.6000e-05 1.1 7.68e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  1  0  0  0  4656
VecCopy                1 1.0 9.7000e-06 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 2 1.0 1.1500e-05 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                2 1.0 3.2300e-05 2.5 1.10e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1359
VecMAXPY              28 1.0 1.6835e-03 1.3 2.22e+06 1.0 0.0e+00 0.0e+00 0.0e+00  0 29  0  0  0   0 29  0  0  0  5280
VecScatterBegin       56 1.0 1.4856e-03 2.7 0.00e+00 0.0 1.7e+02 1.2e+04 0.0e+00  0  0 88 94  0   0  0 88 94  0     0
VecScatterEnd         56 1.0 4.5631e-03 4.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize          28 1.0 1.1202e-03 1.7 2.30e+05 1.0 0.0e+00 0.0e+00 2.8e+01  0  3  0  0 29   0  3  0  0 35   823
SFSetGraph             2 1.0 5.3600e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                2 1.0 1.7310e-04 1.1 0.00e+00 0.0 1.2e+01 3.0e+03 2.0e+00  0  0  6  2  2   0  0  6  2  2     0
SFPack                56 1.0 7.8820e-04 14.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack              56 1.0 3.9600e-05 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               1 1.0 1.3510e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 3.5525e-01 1.0 7.97e+06 1.1 1.8e+02 1.2e+04 6.0e+01 12 98 95 100 61  12 98 95 100 75    85
KSPGMRESOrthog        27 1.0 5.1569e-03 1.1 4.15e+06 1.0 0.0e+00 0.0e+00 2.7e+01  0 54  0  0 28   0 54  0  0 34  3217
PCSetUp                1 1.0 3.2149e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  1  0  0  0  2   1  0  0  0  2     0
PCApply               28 1.0 3.4138e-01 1.0 4.61e+05 0.0 1.0e+02 1.9e+04 5.0e+00 11  2 53 92  5  11  2 53 92  6     1
------------------------------------------------------------------------------------------------------------------------

Object Type          Creations   Destructions. Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     5              5
              Vector    41             41
           Index Set     4              4
   Star Forest Graph     4              4
       Krylov Solver     1              1
      Preconditioner     1              1
              Viewer     1              1
    Distributed Mesh     1              1
     Discrete System     1              1
           Weak Form     1              1
========================================================================================================================
Average time to get PetscTime(): 6e-08
Average time for MPI_Barrier(): 1.66e-06
Average time for zero size MPI_Send(): 2.725e-06
#PETSc Option Table entries:
-aij_only # (source: command line)
-fin /mnt/c/Users/xu/petsc/share/petsc/datafiles/matrices/MYMAT/bcsstk17.mtx # (source: command line)
-ksp_view # (source: command line)
-log_view # (source: command line)
-memory_view # (source: command line)
-pc_factor_mat_solver_type mumps # (source: command line)
-pc_type cholesky # (source: command line)
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-mumps --download-metis --download-parmetis --download-ptscotch --download-superlu --download-superlu_dist --download-strumpack --download-bison --download-scalapack --download-suitesparse --download-cmake --download-fblaslapack --with-debugging=0 PETSC_ARCH=arch-opt
-----------------------------------------
Libraries compiled on 2024-04-15 09:54:45 on DESKTOP-ME2409Q 
Machine characteristics: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
Using PETSc directory: /mnt/c/Users/xu/petsc
Using PETSc arch: arch-opt
-----------------------------------------

Using C compiler: mpicc  -fPIC -Wall -Wwrite-strings -Wno-unknown-pragmas -Wno-lto-type-mismatch -Wno-stringop-overflow -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/mnt/c/Users/xu/petsc/include -I/mnt/c/Users/xu/petsc/arch-opt/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -lpetsc -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -L/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/11 -L/usr/lib/gcc/x86_64-linux-gnu/11 -lspqr -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -ldmumps -lmumps_common -lpord -lpthread -lstrumpack -lscalapack -lsuperlu -lsuperlu_dist -lflapack -lfblas -lptesmumps -lptscotchparmetisv3 -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lparmetis -lmetis -lm -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lopen-rte -lopen-pal -lhwloc -levent_core -levent_pthreads -lgfortran -lm -lz -lgfortran -lm -lgfortran -lgcc_s -lquadmath -lstdc++ -lrt -lquadmath
-----------------------------------------



#####################################
############# SuperLU LU ############
#####################################
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
Reading matrix completes.
KSP Object: 4 MPI processes
  type: gmres
    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
    happy breakdown tolerance 1e-30
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 4 MPI processes
  type: lu
    out-of-place factorization
    tolerance for zero pivot 2.22045e-14
    matrix ordering: external
    factor fill ratio given 0., needed 0.
      Factored matrix follows:
        Mat Object: 4 MPI processes
          type: superlu_dist
          rows=10974, cols=10974
          package used to perform factorization: superlu_dist
          total: nonzeros=0, allocated nonzeros=0
            SuperLU_DIST run parameters:
              Process grid nprow 2 x npcol 2 
              Equilibrate matrix TRUE 
              Replace tiny pivots FALSE 
              Use iterative refinement FALSE 
              Processors in row 2 col partition 2 
              Row permutation LargeDiag_MC64
              Column permutation METIS_AT_PLUS_A
              Parallel symbolic factorization FALSE 
              Repeated factorization SamePattern
  linear system matrix = precond matrix:
  Mat Object: 4 MPI processes
Norm of error 1.2606e-12, Iterations 1
Norm of error 1.2606e-12, Iterations 1
    type: mpiaij
    rows=10974, cols=10974
    total: nonzeros=219812, allocated nonzeros=219812
    total number of mallocs used during MatSetValues calls=0
      not using I-node (on process 0) routines
Norm of error 1.2606e-12, Iterations 1
Norm of error 1.2606e-12, Iterations 1
Summary of Memory Usage in PETSc
Maximum (over computational time) process memory:        total 2.1476e+08 max 5.6234e+07 min 5.2212e+07
Current process memory:                                  total 2.1449e+08 max 5.6234e+07 min 5.1941e+07
****************************************************************************************************************************************************************
***                                WIDEN YOUR WINDOW TO 160 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document                                 ***
****************************************************************************************************************************************************************

------------------------------------------------------------------ PETSc Performance Summary: ------------------------------------------------------------------

./ex23 on a arch-opt named DESKTOP-ME2409Q with 4 processors, by xu Sun Apr 21 22:14:56 2024
Using Petsc Release Version 3.20.5, unknown 

                         Max       Max/Min     Avg       Total
Time (sec):           2.910e+00     1.000   2.910e+00
Objects:              0.000e+00     0.000   0.000e+00
Flops:                3.026e+05     1.332   2.637e+05  1.055e+06
Flops/sec:            1.040e+05     1.332   9.061e+04  3.624e+05
MPI Msg Count:        4.000e+00     2.000   3.000e+00  1.200e+01
MPI Msg Len (bytes):  5.594e+03     2.473   1.309e+03  1.571e+04
MPI Reductions:       4.100e+01     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.9103e+00 100.0%  1.0548e+06 100.0%  1.200e+01 100.0%  1.309e+03      100.0%  2.300e+01  56.1%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          3 1.0 5.3998e-03 91.7 0.00e+00 0.0 3.0e+00 4.0e+00 3.0e+00  0  0 25  0  7   0  0 25  0 13     0
BuildTwoSidedF         2 1.0 5.3912e-03 141.5 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  5   0  0  0  0  9     0
MatMult                2 1.0 1.1014e-03 3.6 2.53e+05 1.4 6.0e+00 2.1e+03 0.0e+00  0 81 50 80  0   0 81 50 80  0   778
MatSolve               2 1.0 1.5353e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
MatLUFactor            1 1.0 3.0888e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 11  0  0  0  0  11  0  0  0  0     0
MatAssemblyBegin       2 1.0 4.4337e-03 93.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  4     0
MatAssemblyEnd         2 1.0 6.0436e-03 1.0 0.00e+00 0.0 6.0e+00 5.3e+02 7.0e+00  0  0 50 20 17   0  0 50 20 30     0
MatGetRowIJ            1 1.0 4.0000e-07 2.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView                2 1.0 1.4608e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  4     0
MatGetLocalMat         1 1.0 1.3095e-03 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot                1 1.0 2.3350e-04 1.9 5.49e+03 1.0 0.0e+00 0.0e+00 1.0e+00  0  2  0  0  2   0  2  0  0  4    94
VecNorm                3 1.0 2.8750e-04 1.6 1.65e+04 1.0 0.0e+00 0.0e+00 3.0e+00  0  6  0  0  7   0  6  0  0 13   229
VecScale               2 1.0 8.3000e-06 1.2 5.49e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  2644
VecCopy                3 1.0 2.1000e-05 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 2 1.0 9.3000e-06 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                2 1.0 1.8300e-05 1.3 1.10e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  2399
VecMAXPY               2 1.0 1.1500e-05 1.5 1.10e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  3817
VecScatterBegin        2 1.0 7.2000e-05 3.9 0.00e+00 0.0 6.0e+00 2.1e+03 0.0e+00  0  0 50 80  0   0  0 50 80  0     0
VecScatterEnd          2 1.0 8.3140e-04 7.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize           2 1.0 2.2690e-04 2.8 1.65e+04 1.0 0.0e+00 0.0e+00 2.0e+00  0  6  0  0  5   0  6  0  0  9   290
SFSetGraph             1 1.0 5.7000e-06 8.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                1 1.0 7.5200e-05 1.4 0.00e+00 0.0 6.0e+00 5.3e+02 1.0e+00  0  0 50 20  2   0  0 50 20  4     0
SFPack                 2 1.0 3.0000e-06 7.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack               2 1.0 8.0000e-07 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               1 1.0 9.0700e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 1.6470e-02 1.0 1.65e+05 1.3 3.0e+00 2.1e+03 3.0e+00  1 55 25 40  7   1 55 25 40 13    35
KSPGMRESOrthog         1 1.0 2.5200e-04 1.8 1.10e+04 1.0 0.0e+00 0.0e+00 1.0e+00  0  4  0  0  2   0  4  0  0  4   174
PCSetUp                1 1.0 3.0911e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00 11  0  0  0  5  11  0  0  0  9     0
PCApply                2 1.0 1.5360e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   1  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Object Type          Creations   Destructions. Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     6              6
              Vector    10             10
           Index Set     2              2
   Star Forest Graph     3              3
       Krylov Solver     1              1
      Preconditioner     1              1
              Viewer     1              1
    Distributed Mesh     1              1
     Discrete System     1              1
           Weak Form     1              1
========================================================================================================================
Average time to get PetscTime(): 3e-08
Average time for MPI_Barrier(): 1.88e-06
Average time for zero size MPI_Send(): 2e-06
#PETSc Option Table entries:
-aij_only # (source: command line)
-fin /mnt/c/Users/xu/petsc/share/petsc/datafiles/matrices/MYMAT/bcsstk17.mtx # (source: command line)
-ksp_view # (source: command line)
-log_view # (source: command line)
-memory_view # (source: command line)
-pc_factor_mat_solver_type superlu_dist # (source: command line)
-pc_type lu # (source: command line)
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-mumps --download-metis --download-parmetis --download-ptscotch --download-superlu --download-superlu_dist --download-strumpack --download-bison --download-scalapack --download-suitesparse --download-cmake --download-fblaslapack --with-debugging=0 PETSC_ARCH=arch-opt
-----------------------------------------
Libraries compiled on 2024-04-15 09:54:45 on DESKTOP-ME2409Q 
Machine characteristics: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
Using PETSc directory: /mnt/c/Users/xu/petsc
Using PETSc arch: arch-opt
-----------------------------------------

Using C compiler: mpicc  -fPIC -Wall -Wwrite-strings -Wno-unknown-pragmas -Wno-lto-type-mismatch -Wno-stringop-overflow -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/mnt/c/Users/xu/petsc/include -I/mnt/c/Users/xu/petsc/arch-opt/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -lpetsc -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -L/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/11 -L/usr/lib/gcc/x86_64-linux-gnu/11 -lspqr -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -ldmumps -lmumps_common -lpord -lpthread -lstrumpack -lscalapack -lsuperlu -lsuperlu_dist -lflapack -lfblas -lptesmumps -lptscotchparmetisv3 -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lparmetis -lmetis -lm -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lopen-rte -lopen-pal -lhwloc -levent_core -levent_pthreads -lgfortran -lm -lz -lgfortran -lm -lgfortran -lgcc_s -lquadmath -lstdc++ -lrt -lquadmath
-----------------------------------------



#####################################
############ Strumpack LU ###########
#####################################
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
Reading matrix completes.
KSP Object: 4 MPI processes
  type: gmres
    restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
    happy breakdown tolerance 1e-30
  maximum iterations=10000, initial guess is zero
  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
  left preconditioning
  using PRECONDITIONED norm type for convergence test
PC Object: 4 MPI processes
  type: lu
    out-of-place factorization
    tolerance for zero pivot 2.22045e-14
    matrix ordering: external
    factor fill ratio given 0., needed 0.
      Factored matrix follows:
        Mat Object: 4 MPI processes
          type: strumpack
          rows=10974, cols=10974
          package used to perform factorization: strumpack
          total: nonzeros=0, allocated nonzeros=0
            STRUMPACK sparse solver!
  linear system matrix = precond matrix:
  Mat Object: 4 MPI processes
Norm of error 1.3158e-12, Iterations 1
Norm of error 1.3158e-12, Iterations 1
    type: mpiaij
    rows=10974, cols=10974
    total: nonzeros=219812, allocated nonzeros=219812
    total number of mallocs used during MatSetValues calls=0
      not using I-node (on process 0) routines
Norm of error 1.3158e-12, Iterations 1
Norm of error 1.3158e-12, Iterations 1
Summary of Memory Usage in PETSc
Maximum (over computational time) process memory:        total 2.0627e+08 max 5.6386e+07 min 4.9017e+07
Current process memory:                                  total 2.0626e+08 max 5.6386e+07 min 4.9017e+07
****************************************************************************************************************************************************************
***                                WIDEN YOUR WINDOW TO 160 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document                                 ***
****************************************************************************************************************************************************************

------------------------------------------------------------------ PETSc Performance Summary: ------------------------------------------------------------------

./ex23 on a arch-opt named DESKTOP-ME2409Q with 4 processors, by xu Sun Apr 21 22:14:59 2024
Using Petsc Release Version 3.20.5, unknown 

                         Max       Max/Min     Avg       Total
Time (sec):           2.863e+00     1.000   2.863e+00
Objects:              0.000e+00     0.000   0.000e+00
Flops:                3.026e+05     1.332   2.637e+05  1.055e+06
Flops/sec:            1.057e+05     1.332   9.212e+04  3.685e+05
MPI Msg Count:        4.000e+00     2.000   3.000e+00  1.200e+01
MPI Msg Len (bytes):  5.594e+03     2.473   1.309e+03  1.571e+04
MPI Reductions:       4.100e+01     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 2.8626e+00 100.0%  1.0548e+06 100.0%  1.200e+01 100.0%  1.309e+03      100.0%  2.300e+01  56.1%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

BuildTwoSided          3 1.0 8.6347e-03 166.1 0.00e+00 0.0 3.0e+00 4.0e+00 3.0e+00  0  0 25  0  7   0  0 25  0 13     0
BuildTwoSidedF         2 1.0 8.6150e-03 165.7 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  5   0  0  0  0  9     0
MatMult                2 1.0 1.4890e-02 40.8 2.53e+05 1.4 6.0e+00 2.1e+03 0.0e+00  0 81 50 80  0   0 81 50 80  0    58
MatSolve               2 1.0 5.8068e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactor            1 1.0 1.7059e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  6  0  0  0  0   6  0  0  0  0     0
MatAssemblyBegin       2 1.0 6.0956e-03 91.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  4     0
MatAssemblyEnd         2 1.0 7.0775e-03 1.0 0.00e+00 0.0 6.0e+00 5.3e+02 7.0e+00  0  0 50 20 17   0  0 50 20 30     0
MatGetRowIJ            1 1.0 4.0000e-07 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView                2 1.0 1.5630e-03 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  2   0  0  0  0  4     0
MatGetLocalMat         1 1.0 8.2480e-04 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecMDot                1 1.0 1.5900e-05 2.7 5.49e+03 1.0 0.0e+00 0.0e+00 1.0e+00  0  2  0  0  2   0  2  0  0  4  1380
VecNorm                3 1.0 1.3290e-04 5.9 1.65e+04 1.0 0.0e+00 0.0e+00 3.0e+00  0  6  0  0  7   0  6  0  0 13   495
VecScale               2 1.0 3.8000e-06 1.1 5.49e+03 1.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  2  0  0  0  5776
VecCopy                1 1.0 1.3800e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 2 1.0 8.6000e-06 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY                2 1.0 1.9900e-05 1.6 1.10e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0  2206
VecMAXPY               2 1.0 3.7000e-06 1.1 1.10e+04 1.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  4  0  0  0 11864
VecScatterBegin        2 1.0 5.0700e-05 1.7 0.00e+00 0.0 6.0e+00 2.1e+03 0.0e+00  0  0 50 80  0   0  0 50 80  0     0
VecScatterEnd          2 1.0 1.4501e-02 659.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize           2 1.0 2.4200e-05 1.0 1.65e+04 1.0 0.0e+00 0.0e+00 2.0e+00  0  6  0  0  5   0  6  0  0  9  2720
SFSetGraph             1 1.0 6.7000e-06 22.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                1 1.0 7.3300e-05 1.7 0.00e+00 0.0 6.0e+00 5.3e+02 1.0e+00  0  0 50 20  2   0  0 50 20  4     0
SFPack                 2 1.0 8.6000e-06 14.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack               2 1.0 1.6000e-06 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               1 1.0 1.1930e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve               1 1.0 6.5144e-03 1.0 1.65e+05 1.3 3.0e+00 2.1e+03 3.0e+00  0 55 25 40  7   0 55 25 40 13    89
KSPGMRESOrthog         1 1.0 2.3700e-05 2.0 1.10e+04 1.0 0.0e+00 0.0e+00 1.0e+00  0  4  0  0  2   0  4  0  0  4  1852
PCSetUp                1 1.0 1.7076e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  6  0  0  0  5   6  0  0  0  9     0
PCApply                2 1.0 5.8141e-03 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
------------------------------------------------------------------------------------------------------------------------

Object Type          Creations   Destructions. Reports information only for process 0.

--- Event Stage 0: Main Stage

              Matrix     6              6
              Vector    10             10
           Index Set     2              2
   Star Forest Graph     3              3
       Krylov Solver     1              1
      Preconditioner     1              1
              Viewer     1              1
    Distributed Mesh     1              1
     Discrete System     1              1
           Weak Form     1              1
========================================================================================================================
Average time to get PetscTime(): 4e-08
Average time for MPI_Barrier(): 1.46e-06
Average time for zero size MPI_Send(): 1.85e-06
#PETSc Option Table entries:
-aij_only # (source: command line)
-fin /mnt/c/Users/xu/petsc/share/petsc/datafiles/matrices/MYMAT/bcsstk17.mtx # (source: command line)
-ksp_view # (source: command line)
-log_view # (source: command line)
-memory_view # (source: command line)
-pc_factor_mat_solver_type strumpack # (source: command line)
-pc_type lu # (source: command line)
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-mumps --download-metis --download-parmetis --download-ptscotch --download-superlu --download-superlu_dist --download-strumpack --download-bison --download-scalapack --download-suitesparse --download-cmake --download-fblaslapack --with-debugging=0 PETSC_ARCH=arch-opt
-----------------------------------------
Libraries compiled on 2024-04-15 09:54:45 on DESKTOP-ME2409Q 
Machine characteristics: Linux-5.15.133.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
Using PETSc directory: /mnt/c/Users/xu/petsc
Using PETSc arch: arch-opt
-----------------------------------------

Using C compiler: mpicc  -fPIC -Wall -Wwrite-strings -Wno-unknown-pragmas -Wno-lto-type-mismatch -Wno-stringop-overflow -fstack-protector -fvisibility=hidden -g -O  
Using Fortran compiler: mpif90  -fPIC -Wall -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -Wno-unused-dummy-argument -g -O    
-----------------------------------------

Using include paths: -I/mnt/c/Users/xu/petsc/include -I/mnt/c/Users/xu/petsc/arch-opt/include
-----------------------------------------

Using C linker: mpicc
Using Fortran linker: mpif90
Using libraries: -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -lpetsc -Wl,-rpath,/mnt/c/Users/xu/petsc/arch-opt/lib -L/mnt/c/Users/xu/petsc/arch-opt/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -L/usr/lib/x86_64-linux-gnu/openmpi/lib/fortran/gfortran -Wl,-rpath,/usr/lib/gcc/x86_64-linux-gnu/11 -L/usr/lib/gcc/x86_64-linux-gnu/11 -lspqr -lumfpack -lklu -lcholmod -lbtf -lccolamd -lcolamd -lcamd -lamd -lsuitesparseconfig -ldmumps -lmumps_common -lpord -lpthread -lstrumpack -lscalapack -lsuperlu -lsuperlu_dist -lflapack -lfblas -lptesmumps -lptscotchparmetisv3 -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lparmetis -lmetis -lm -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lopen-rte -lopen-pal -lhwloc -levent_core -levent_pthreads -lgfortran -lm -lz -lgfortran -lm -lgfortran -lgcc_s -lquadmath -lstdc++ -lrt -lquadmath
-----------------------------------------




#####################################
########### SuiteSparse LU ##########
#####################################
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
Reading matrix completes.


#####################################
########### SuiteSparse Cho #########
#####################################
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
Reading matrix completes.
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric


#####################################
########### SuiteSparse QR ##########
#####################################
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
Reading matrix completes.
%%MatrixMarket matrix coordinate real symmetric


#####################################
############## Petsc LU #############
#####################################
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
Reading matrix completes.


######################################
############## Petsc Cho #############
######################################
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
%%MatrixMarket matrix coordinate real symmetric
Reading matrix completes.


